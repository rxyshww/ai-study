{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_openai langchain_community langchain_core -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from google.colab import userdata\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_BASE\"] = userdata.get('OPENAI_API_BASE')\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "question = \"甘肃天水麻辣烫是什么时候火的，为什么会火\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"qwen2-72b-instruct\", temperature=0)\n",
    "\n",
    "res = llm.invoke([\n",
    "    SystemMessage(\"你是一个智能助手\"),\n",
    "    HumanMessage(question)\n",
    "])\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from google.colab import userdata\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_BASE\"] = userdata.get('OPENAI_API_BASE')\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
    "\n",
    "question = \"甘肃天水麻辣烫是什么时候火的，为什么会火\"\n",
    "\n",
    "tool = TavilySearchResults(\n",
    "    max_results=5,\n",
    "    search_depth=\"advanced\",\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    ")\n",
    "\n",
    "search = tool.invoke({\"query\": question})\n",
    "\n",
    "llm = ChatOpenAI(model=\"qwen2-72b-instruct\", temperature=0)\n",
    "\n",
    "res = llm.invoke([\n",
    "    SystemMessage(\"你是一个智能助手，请根据上下文回答问题，上下文是:\" + \"\\n\".join([x['content'] for x in search])),\n",
    "    HumanMessage(question)\n",
    "])\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from google.colab import userdata\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_BASE\"] = userdata.get('OPENAI_API_BASE')\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
    "\n",
    "\n",
    "question = \"特朗普是几年几月几日在演讲中被袭击打中耳朵\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"qwen2-72b-instruct\", temperature=0)\n",
    "\n",
    "tool = TavilySearchResults(\n",
    "    max_results=5,\n",
    "    search_depth=\"advanced\",\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    ")\n",
    "\n",
    "search = tool.invoke({\"query\": question})\n",
    "\n",
    "res = llm.invoke([\n",
    "    SystemMessage(\"你是一个智能助手\"),\n",
    "    HumanMessage(question)\n",
    "])\n",
    "\n",
    "res2 = llm.invoke([\n",
    "    SystemMessage(\"你是一个智能助手，请根据上下文回答问题，上下文是:\" + \"\\n\".join([x['content'] for x in search])),\n",
    "    HumanMessage(question)\n",
    "])\n",
    "\n",
    "print(search)\n",
    "\n",
    "print(\"----------------分割线----------------\")\n",
    "\n",
    "print(res.content)\n",
    "\n",
    "print(\"----------------分割线----------------\")\n",
    "\n",
    "print(res2.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
