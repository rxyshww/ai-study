{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_openai langchain_community langchain_core -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function call 简介\n",
    "\n",
    "LLM的Function Call（函数调用）是一种功能，允许大型语言模型（如GPT）在生成文本的过程中调用外部函数或服务。通过这种方式，开发者可以定义各种函数，使得LLM能够根据对话的上下文和需求进行调用。这些函数可以用于数据提取、知识检索和API集成等任务，从而增强LLM的能力。\n",
    "具体来说，Function Call的作用包括：\n",
    "\n",
    "- 准确识别用户意图：将用户的语义转化为结构化的指令，从而准确判断需要调用哪个函数及其参数。\n",
    "- 增强对话能力：使得对话代理或聊天机器人能够处理复杂问题，通过调用外部API或知识库提供更相关和有用的响应。\n",
    "- 解决模型的局限性：通过引入工具插件来解决模型在时间处理和专业知识方面的缺陷，如幻觉问题。\n",
    "总结来说：function call能够让你用很少的提示词，让llm输出更加准确，数据结构更加稳定。\n",
    "\n",
    "我们还是用几个场景示例来说明，需要实现的功能如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 官网天气示例\n",
    "试想一下，如果我们询问北京天气，我们要从一段内容中提取参数location，并调用get_current_weather，获取结果，需要怎么做。在上一小节中，我们学习了json mode能力，可以写提示词，让ai帮我们提取包含location的对象，然后再传给函数得到结果，使用function calling该怎么实现呢。\n",
    "\n",
    "> 我们使用function_calling，把这个函数的描述告诉大模型，它就会返回给你这个函数所需要的参数。这个过程就会变得简洁、可控，结果更加准确。使用过程如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = userdata.get('OPENAI_API_BASE')\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "model = ChatOpenAI(model=\"qwen2.5-72b-instruct\").bind_tools(tools)\n",
    "res = model.invoke(\"北京的天气怎么样\")\n",
    "print(res.tool_calls[0][\"name\"])\n",
    "print()\n",
    "print(res.tool_calls[0][\"args\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数学运算示例\n",
    "\n",
    "上一个例子中，我们手写了json_schema来描述函数。但是，我们不能每次都自己手动做这件事，这样成本会变得很高。langchain提供了tool修饰符，可以增强工具，并允许大模型来绑定这些工具。在调用大模型时，自动帮我们生成json_schema。\n",
    "\n",
    "有一个计算函数，它接受两个整数参数 a 和 b，并返回它们的乘积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amultiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from google.colab import userdata\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 定义一个工具函数，用于计算两个数的乘积\n",
    "@tool\n",
    "def amultiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_BASE\"] = userdata.get('OPENAI_API_BASE')\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(model=\"qwen2.5-72b-instruct\", temperature=0)\n",
    "\n",
    "# 将定义的工具函数绑定到模型上\n",
    "ll_tool = llm.bind_tools([amultiply])\n",
    "\n",
    "# 调用模型，传入参数并获取结果\n",
    "result = ll_tool.invoke('a 为 3 b 是 6')\n",
    "\n",
    "# 检查结果中是否包含工具调用，并打印相关信息\n",
    "if hasattr(result, 'tool_calls') and result.tool_calls:\n",
    "    print(result.tool_calls[0]['args'])  # 打印工具调用的参数\n",
    "    print(type(result.tool_calls[0]['args']))  # 打印参数的类型\n",
    "    \n",
    "    # 调用 amultiply 工具函数，传入解析后的参数并打印结果\n",
    "    print(amultiply.invoke(result.tool_calls[0]['args']))\n",
    "else:\n",
    "    print(result.content)  # 如果没有工具调用，则打印内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.tools import tool\n",
    "from google.colab import userdata\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_BASE\"] = userdata.get('OPENAI_API_BASE')\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "    \n",
    "idl1 = \"\"\"\n",
    "message ToolRequest {\n",
    "    string tool_id = 1;  // 工具的唯一标识符\n",
    "    int32 version = 2;   // 工具的版本号\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "idl2 = \"\"\"\n",
    "message ToolRequest {\n",
    "    string tool_name = 1;  // 工具的名称\n",
    "    int32 version = 2;     // 工具的版本号\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "idl3 = \"\"\"\n",
    "message ToolRequest {\n",
    "    string tool_category = 1;  // 工具的类别\n",
    "    int32 version = 2;         // 工具的版本号\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "idl4 = \"\"\"\n",
    "message ToolRequest {\n",
    "    string tool_description = 1;  // 工具的描述信息\n",
    "    int32 version = 2;            // 工具的版本号\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "class InterfaceDefinition(TypedDict):\n",
    "    path: str\n",
    "    description: str\n",
    "\n",
    "class PSMDefinition(TypedDict):\n",
    "    name: str\n",
    "    interfaces: List[InterfaceDefinition]\n",
    "    \n",
    "\n",
    "interfaceMap: List[PSMDefinition] = [\n",
    "    {\n",
    "        \"name\": \"anchor.psm1\",\n",
    "        \"interfaces\": [\n",
    "            {\n",
    "                \"path\": \"/ark/anchor1\",\n",
    "                \"idl\": idl1\n",
    "            },\n",
    "            {\n",
    "                \"path\": \"/ark/anchor2\",\n",
    "                \"idl\": idl2\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"webcast.psm2\",\n",
    "        \"interfaces\": [\n",
    "            {\n",
    "                \"path\": \"/webcast/tool1\",\n",
    "                \"idl\": idl3\n",
    "            },\n",
    "            {\n",
    "                \"path\": \"/webcast/tool2\",\n",
    "                \"idl\": idl4\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "def find_idl_by_name_and_path(psm, path):\n",
    "    for psmInfo in interfaceMap:\n",
    "        if psmInfo[\"name\"] == psm:\n",
    "            for interface in psmInfo[\"interfaces\"]:\n",
    "                if interface[\"path\"] == path:\n",
    "                    return interface[\"idl\"]\n",
    "\n",
    "class ConfigDict(BaseModel):\n",
    "    path: str = Field(description=\"包含 path 和 psm 的字典\")\n",
    "    psm: str = Field(description=\"接口对应的psm，示例：aaa.bbb.ccc，字母和.组成\")\n",
    "\n",
    "class ExplainKeyByPathInput(BaseModel):\n",
    "    queryInfo: ConfigDict = Field(description=\"包含 path 和 psm 的字典\")\n",
    "    keyName: str = Field(description=\"字段名称\")\n",
    "\n",
    "@tool(\"explain-key-by-path\", args_schema=ExplainKeyByPathInput)\n",
    "def explain_key_by_path(\n",
    "    queryInfo: ConfigDict,  # 定义一个包含 path 和 psm 的字典\n",
    "    keyName: Annotated[str, \"字段名称\"],\n",
    "):\n",
    "    \"\"\"字段解释工具，通过接口地址和接口对应的psm查找接口定义，并根据字段名来获取字段说明\"\"\"\n",
    "    # 你可以选择不使用 config 参数\n",
    "    print(queryInfo)\n",
    "    idl = find_idl_by_name_and_path(**queryInfo.dict())  # 将 ConfigDict 转换为字典\n",
    "    \n",
    "    return idl\n",
    "\n",
    "print(explain_key_by_path.args_schema.schema())\n",
    "\n",
    "llm2 = ChatOpenAI(model=\"qwen2.5-72b-instruct\", temperature=0)\n",
    "\n",
    "llm_tool = llm2.bind_tools([explain_key_by_path])\n",
    "\n",
    "result = llm_tool.invoke('查询psm:webcast.psm2 接口 /webcast/tool2 中的 tool_name字段')\n",
    "\n",
    "if hasattr(result, 'tool_calls') and result.tool_calls:\n",
    "    print(result.tool_calls[0]['args'])\n",
    "    print(explain_key_by_path.invoke(result.tool_calls[0]['args']))\n",
    "else:\n",
    "    print(result.content)\n",
    "\n",
    "\n",
    "\n",
    "# # 示例使用\n",
    "# name = \"anchor.psm1\"\n",
    "# path = \"/ark/anchor2\"\n",
    "# idl = find_idl_by_name_and_path(name, path)\n",
    "# print(idl)  # 输出: idl1\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
