{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_openai langchain -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llm本身是没有记忆的\n",
    "\n",
    "我们通过例子直观体验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from google.colab import userdata\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = userdata.get('OPENAI_API_BASE')\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(model=\"qwen2-72b-instruct\", temperature=0)\n",
    "\n",
    "response1 = llm.invoke([\n",
    "  HumanMessage(content=\"你好，能给我推荐一本学习Python的书吗？\")\n",
    "])\n",
    "\n",
    "response2 = llm.invoke([\n",
    "  HumanMessage(content=\"这本书怎么样？\")\n",
    "])\n",
    "\n",
    "print(response1)\n",
    "print(\"====================\")\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 让LLM拥有记忆\n",
    "\n",
    "首先，我们通过简单的方法，将首次咨询大模型的结果，在第二次咨询时一并传递给大模型。这样，当我们询问这本书怎么样时，其便能理解我们的提问，并基于历史记录作出回答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from google.colab import userdata\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = userdata.get('OPENAI_API_BASE')\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(model=\"qwen2-72b-instruct\", temperature=0)\n",
    "\n",
    "response1 = llm.invoke([\n",
    "  HumanMessage(content=\"你好，能给我推荐一本学习Python的书吗？\")\n",
    "])\n",
    "\n",
    "response2 = llm.invoke([\n",
    "  HumanMessage(content=\"你好，能给我推荐一本学习Python的书吗？\"),\n",
    "  AIMessage(content=response1.content),\n",
    "  HumanMessage(content=\"这本书怎么样？\")\n",
    "])\n",
    "\n",
    "print(response1)\n",
    "print(\"====================\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### langchain内置的记忆存储方法\n",
    "\n",
    "可以使用`ConversationBufferMemory`来实现记忆，不过这种方式记忆清理，多用户等不是很好控制，所以不常用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from google.colab import userdata\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = userdata.get('OPENAI_API_BASE')\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "chat = ChatOpenAI(model=\"qwen2-72b-instruct\", temperature=0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=chat, memory=memory)\n",
    "\n",
    "response1 = conversation.predict(input=\"你好，能给我推荐一本学习Python的书吗？\")\n",
    "response2 = conversation.predict(input=\"这本书怎么样？\")\n",
    "\n",
    "print(response1)\n",
    "print(\"====================\")\n",
    "print(response2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
